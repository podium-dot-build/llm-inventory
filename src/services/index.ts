import type { ModelDatabase, ModelInfo } from "@/types";
import { calculatePerTokenPrice } from "@/utils/calculatePerTokenPrice";

/**
 * Database of AI models information
 */
export const modelInfo: ModelDatabase = {
	// Anthropic Claude 3 Family
	"claude-3-haiku": {
		id: "claude-3-haiku",
		name: "Claude 3 Haiku",
		description: "Fastest and most compact model for near-instant responsiveness",
		context: 200_000,
		outputLimit: 4_096,
		inputCost: 0.25,
		outputCost: 1.25,
		strengths: "Quick and accurate targeted performance",
		supports: ["text", "chat", "image"],
	},
	"claude-3-opus": {
		id: "claude-3-opus",
		name: "Claude 3 Opus",
		description: "Powerful model for complex tasks",
		context: 200_000,
		outputLimit: 4_096,
		inputCost: 15.0,
		outputCost: 75.0,
		strengths: "Top-level intelligence, fluency, and understanding",
		supports: ["text", "chat", "image"],
	},
	"claude-3-5-haiku": {
		id: "claude-3-5-haiku",
		name: "Claude 3.5 Haiku",
		description: "Our fastest model",
		context: 200_000,
		outputLimit: 8_192,
		inputCost: 0.8,
		outputCost: 4.0,
		strengths: "Intelligence at blazing speeds",
		supports: ["text", "chat", "image"],
	},
	"claude-3-5-sonnet": {
		id: "claude-3-5-sonnet",
		name: "Claude 3.5 Sonnet",
		description: "High-capability model balancing intelligence and speed",
		context: 200_000,
		outputLimit: 8_192,
		inputCost: 3.0,
		outputCost: 15.0,
		strengths: "High level of intelligence and capability",
		supports: ["text", "chat", "image"],
	},
	"claude-3-7-sonnet": {
		id: "claude-3-7-sonnet",
		name: "Claude 3.7 Sonnet",
		description: "Our most intelligent model with extended thinking mode",
		context: 200_000,
		outputLimit: 64_000,
		inputCost: 3.0,
		outputCost: 15.0,
		strengths:
			"Highest intelligence and capability with toggleable extended reasoning",
		supports: ["text", "chat", "image"],
	},

	// Perplexity Sonar Family
	perplexity_sonar_pro: {
		id: "perplexity_sonar_pro",
		name: "Sonar Pro",
		description: "Advanced search-focused LLM for deep, cited answers",
		context: 200_000,
		outputLimit: 8_000,
		inputCost: 3.0,
		outputCost: 15.0,
		strengths: "In-depth answers with extensive citations and grounded reasoning",
		supports: ["text", "chat"],
	},
	perplexity_sonar: {
		id: "perplexity_sonar",
		name: "Sonar",
		description: "Cost-effective search model for quick, grounded Q&A",
		context: 128_000,
		outputLimit: 4_096, // typical concise answer length
		inputCost: 1.0,
		outputCost: 1.0,
		strengths: "Fast factual queries with real-time citations",
		supports: ["text", "chat"],
	},
	perplexity_sonar_reasoning_pro: {
		id: "perplexity_sonar_reasoning_pro",
		name: "Sonar Reasoning Pro",
		description: "High-performance reasoning with chain-of-thought and search",
		context: 128_000,
		outputLimit: 4_096,
		inputCost: 2.0,
		outputCost: 8.0,
		strengths: "Deep chain-of-thought analysis with integrated retrieval",
		supports: ["text", "chat"],
	},
	perplexity_sonar_reasoning: {
		id: "perplexity_sonar_reasoning",
		name: "Sonar Reasoning",
		description: "Reasoning-centric model with real-time search support",
		context: 128_000,
		outputLimit: 4_096,
		inputCost: 1.0,
		outputCost: 5.0,
		strengths: "Step-by-step logical reasoning with citations",
		supports: ["text", "chat"],
	},
	perplexity_sonar_deep_research: {
		id: "perplexity_sonar_deep_research",
		name: "Sonar Deep Research",
		description: "Expert-level research model for exhaustive analysis",
		context: 128_000,
		outputLimit: 16_384, // supports longer reports
		inputCost: 2.0,
		outputCost: 8.0,
		strengths: "Comprehensive white-paper style outputs from multiple sources",
		supports: ["text", "chat"],
	},
	perplexity_r1_1776: {
		id: "perplexity_r1_1776",
		name: "R1-1776 (Offline)",
		description:
			"Offline chat model providing factual answers from internal knowledge",
		context: 128_000,
		outputLimit: 8_192,
		inputCost: 2.0,
		outputCost: 8.0,
		strengths: "Consistent, uncensored responses without external search",
		supports: ["text", "chat"],
	},

	// Mistral AI Models
	mistral_7b_base: {
		id: "mistral_7b_base",
		name: "Mistral 7B",
		description:
			"Open-source 7.3B-parameter model outperforming much larger baselines",
		context: 8_192,
		outputLimit: 8_192,
		inputCost: 0.0,
		outputCost: 0.0,
		strengths: "Strong performance on benchmarks, efficient inference",
		supports: ["text"],
	},
	mistral_7b_instruct: {
		id: "mistral_7b_instruct",
		name: "Mistral 7B Instruct",
		description: "Instruction-tuned variant of Mistral 7B for chat and tasks",
		context: 8_192,
		outputLimit: 8_192,
		inputCost: 0.0,
		outputCost: 0.0,
		strengths: "High-quality instruction following, chat-optimized",
		supports: ["text", "chat"],
	},
	mistral_nemo_12b_base: {
		id: "mistral_nemo_12b_base",
		name: "Mistral NeMo 12B",
		description: "Multilingual model with 128K context, strong reasoning and code",
		context: 128_000,
		outputLimit: 16_384,
		inputCost: 0.0,
		outputCost: 0.0,
		strengths: "State-of-the-art multilingual and coding performance",
		supports: ["text", "chat", "code"],
	},
	mistral_nemo_12b_instruct: {
		id: "mistral_nemo_12b_instruct",
		name: "Mistral NeMo 12B Instruct",
		description: "Instruction-tuned 12B-parameter model for dialogue and tasks",
		context: 128_000,
		outputLimit: 16_384,
		inputCost: 0.0,
		outputCost: 0.0,
		strengths: "Aligned for multi-turn dialogue and instruction following",
		supports: ["text", "chat", "code"],
	},
	mistral_mixtral_8x7b: {
		id: "mistral_mixtral_8x7b",
		name: "Mixtral 8×7B",
		description:
			"Sparse Mixture-of-Experts model matching 70B performance at 6× speed",
		context: 32_768,
		outputLimit: 32_768,
		inputCost: 0.0,
		outputCost: 0.0,
		strengths: "High efficiency MoE, strong coding and reasoning",
		supports: ["text", "chat"],
	},
	mistral_mixtral_8x7b_instruct: {
		id: "mistral_mixtral_8x7b_instruct",
		name: "Mixtral 8×7B Instruct",
		description: "Instruction-tuned MoE model for helpful, controlled responses",
		context: 32_768,
		outputLimit: 32_768,
		inputCost: 0.0,
		outputCost: 0.0,
		strengths: "Top open-source instruction quality, GPT-3.5-level alignment",
		supports: ["text", "chat"],
	},

	// xAI Grok Models
	xai_grok_3_beta: {
		id: "xai_grok_3_beta",
		name: "Grok 3 (Beta)",
		description: "Enterprise-scale model for long-document analysis and code",
		context: 131_072,
		outputLimit: 32_768,
		inputCost: 3.0,
		outputCost: 15.0,
		strengths: "Complex reasoning, data extraction, code assistance",
		supports: ["text", "chat"],
	},
	xai_grok_3_mini_beta: {
		id: "xai_grok_3_mini_beta",
		name: "Grok 3 Mini (Beta)",
		description: "Lightweight model optimized for math and structured reasoning",
		context: 131_072,
		outputLimit: 16_384,
		inputCost: 0.3,
		outputCost: 0.5,
		strengths: "Efficient quantitative reasoning with high accuracy",
		supports: ["text", "chat"],
	},
	xai_grok_2_vision_1212: {
		id: "xai_grok_2_vision_1212",
		name: "Grok 2 Vision (1212)",
		description: "Multimodal model processing images+text for grounded answers",
		context: 8_192,
		outputLimit: 4_096,
		inputCost: 2.0,
		outputCost: 10.0,
		strengths: "Image understanding and integrated visual reasoning",
		supports: ["text", "image"],
	},
	xai_grok_2_image_1212: {
		id: "xai_grok_2_image_1212",
		name: "Grok 2 Image (1212)",
		description: "Image generation model producing novel images from text prompts",
		context: 131_072,
		outputLimit: 0, // images, not token-based
		inputCost: 0.0,
		outputCost: 0.07, // per image
		strengths: "High-quality, multi-style image generation",
		supports: ["image"],
	},

	// Groq Llama-3 Tool-Use Models
	groq_llama3_70b_tool_use: {
		id: "groq_llama3_70b_tool_use",
		name: "Llama-3-Groq 70B (Tool-Use)",
		description: "Meta Llama 3 fine-tuned for function calls and tool chains",
		context: 128_000,
		outputLimit: 32_768,
		inputCost: 0.0,
		outputCost: 0.0,
		strengths: "Superior tool-use accuracy and structured outputs",
		supports: ["text", "chat"],
	},
	groq_llama3_8b_tool_use: {
		id: "groq_llama3_8b_tool_use",
		name: "Llama-3-Groq 8B (Tool-Use)",
		description: "Compact function-calling model with low latency",
		context: 128_000,
		outputLimit: 16_384,
		inputCost: 0.0,
		outputCost: 0.0,
		strengths: "Efficient multi-step reasoning with tool integration",
		supports: ["text", "chat"],
	},

	// Inception Mercury Coder
	inception_mercury_coder_small: {
		id: "inception_mercury_coder_small",
		name: "Mercury Coder Small",
		description: "Diffusion LLM optimized for ultra-fast code generation",
		context: 8_192,
		outputLimit: 8_192,
		inputCost: 0.0,
		outputCost: 0.0,
		strengths: "737 tokens/sec code throughput, high accuracy",
		supports: ["text"],
	},
	inception_mercury_coder_mini: {
		id: "inception_mercury_coder_mini",
		name: "Mercury Coder Mini",
		description: "Even faster diffusion coder for lightweight hardware",
		context: 8_192,
		outputLimit: 8_192,
		inputCost: 0.0,
		outputCost: 0.0,
		strengths: "1,109 tokens/sec, excellent for real-time dev assistance",
		supports: ["text"],
	},

	// Google Gemini Series
	google_gemini_pro: {
		id: "google_gemini_pro",
		name: "Gemini Pro (1.5)",
		description: "General-purpose multimodal model with up to 2M context",
		context: 2_000_000,
		outputLimit: 64_000,
		inputCost: 1.25,
		outputCost: 5.0,
		strengths: "SOTA reasoning, coding, and vision; MMLU >90%",
		supports: ["text", "chat", "image", "code"],
	},
	google_gemini_flash: {
		id: "google_gemini_flash",
		name: "Gemini Flash",
		description: "High-throughput multimodal model for large-scale tasks",
		context: 1_000_000,
		outputLimit: 32_768,
		inputCost: 0.075,
		outputCost: 0.3,
		strengths: "Fast, cost-effective generation with 1M memory",
		supports: ["text", "chat", "image"],
	},
	google_gemini_flash_8b: {
		id: "google_gemini_flash_8b",
		name: "Gemini Flash 8B",
		description: "Lightweight Gemini variant with 1M token context",
		context: 1_000_000,
		outputLimit: 16_384,
		inputCost: 0.0375,
		outputCost: 0.15,
		strengths: "Low-cost chat and text tasks with huge memory",
		supports: ["text", "chat", "image"],
	},
	google_gemini_ultra: {
		id: "google_gemini_ultra",
		name: "Gemini Ultra (Preview)",
		description: "Next-gen multimodal model in limited preview",
		context: 131_072,
		outputLimit: 64_000,
		inputCost: 0.0, // preview
		outputCost: 0.0, // preview
		strengths: "Exceeds GPT-4 on vision and multimodal benchmarks",
		supports: ["text", "chat", "image"],
	},

	// Fireworks AI
	fireworks_f1: {
		id: "fireworks_f1",
		name: "Fireworks f1",
		description: "Compound multi-model system beating GPT-4o & Claude 3.5",
		context: 1_000_000,
		outputLimit: 16_384,
		inputCost: 0.0,
		outputCost: 0.0,
		strengths: "Exceptional multi-step reasoning via ensemble inference",
		supports: ["text", "chat"],
	},
	fireworks_f1_mini: {
		id: "fireworks_f1_mini",
		name: "Fireworks f1-mini",
		description: "Lighter compound model for efficient reasoning",
		context: 1_000_000,
		outputLimit: 8_192,
		inputCost: 0.0,
		outputCost: 0.0,
		strengths: "Compact ensemble reasoning with good accuracy",
		supports: ["text", "chat"],
	},
	fireworks_llama4_maverick: {
		id: "fireworks_llama4_maverick",
		name: "Llama 4 Maverick",
		description: "Meta MoE multimodal model with 1M token memory",
		context: 1_000_000,
		outputLimit: 64_000,
		inputCost: 0.22,
		outputCost: 0.88,
		strengths: "Expert routing, SOTA multimodal context handling",
		supports: ["text", "chat", "image"],
	},
	fireworks_llama4_scout: {
		id: "fireworks_llama4_scout",
		name: "Llama 4 Scout",
		description: "General-purpose variant with 10M context chain capability",
		context: 1_000_000,
		outputLimit: 64_000,
		inputCost: 0.15,
		outputCost: 0.6,
		strengths: "Ideal for multi-document summarization and analysis",
		supports: ["text", "chat", "image"],
	},

	// DeepSeek (R1)
	deepseek_v3_r1: {
		id: "deepseek_v3_r1",
		name: "DeepSeek v3 / R1",
		description: "Sparse MoE model powering Perplexity’s top reasoning",
		context: 128_000,
		outputLimit: 8_192,
		inputCost: 2.0,
		outputCost: 8.0,
		strengths: "Efficient MoE reasoning with offline/online modes",
		supports: ["text", "chat"],
	},

	// DeepInfra Models
	deepinfra_deepseek_prover_v2: {
		id: "deepinfra_deepseek_prover_v2",
		name: "DeepSeek Prover v2",
		description: "671B-parameter model for formal logic and proofs",
		context: 164_000,
		outputLimit: 16_384,
		inputCost: 0.7,
		outputCost: 2.18,
		strengths: "State-of-the-art mathematical reasoning and proofs",
		supports: ["text", "chat"],
	},
	deepinfra_llama_guard_4: {
		id: "deepinfra_llama_guard_4",
		name: "Llama Guard 4",
		description: "12B-parameter multimodal content moderation model",
		context: 164_000,
		outputLimit: 1_024, // short classification outputs
		inputCost: 0.05,
		outputCost: 0.05,
		strengths: "Robust multilingual text/image safety classification",
		supports: ["text", "image"],
	},

	// Cohere Command Series
	cohere_command_r_plus: {
		id: "cohere_command_r_plus",
		name: "Command R+",
		description: "Enterprise-grade instruction model with 128K context",
		context: 128_000,
		outputLimit: 16_384,
		inputCost: 3.0,
		outputCost: 15.0,
		strengths: "Exceptional long-form instruction following and RAG",
		supports: ["text", "chat"],
	},
	cohere_command_r: {
		id: "cohere_command_r",
		name: "Command R",
		description: "Versatile instruction model with big context at lower cost",
		context: 128_000,
		outputLimit: 16_384,
		inputCost: 0.5,
		outputCost: 1.5,
		strengths: "High throughput instruction following, cost-effective",
		supports: ["text", "chat"],
	},

	// Amazon Titan Text Family (Bedrock)
	amazon_titan_text_premier_v1: {
		id: "amazon_titan_text_premier_v1",
		name: "Titan Text Premier",
		description: "32K context LLM for code, chat, and summarization",
		context: 32_000,
		outputLimit: 32_000,
		inputCost: 0.8, // $0.0008 per 1K → $0.8 per 1M
		outputCost: 1.6, // $0.0016 per 1K → $1.6 per 1M
		strengths: "High-capacity text generation and RAG support",
		supports: ["text", "chat"],
	},
	amazon_titan_text_express_v1: {
		id: "amazon_titan_text_express_v1",
		name: "Titan Text Express",
		description: "8K context LLM for open-ended chat and summarization",
		context: 8_000,
		outputLimit: 8_000,
		inputCost: 0.8,
		outputCost: 1.6,
		strengths: "Fast, cost-efficient chat with retrieval support",
		supports: ["text", "chat"],
	},
	amazon_titan_text_lite_v1: {
		id: "amazon_titan_text_lite_v1",
		name: "Titan Text Lite",
		description: "4K context lightweight model for chat and copywriting",
		context: 4_000,
		outputLimit: 4_000,
		inputCost: 0.8,
		outputCost: 1.6,
		strengths: "Efficient, customizable for fine-tuning tasks",
		supports: ["text", "chat"],
	},
};

/**
 * Get a model by ID
 */
export const getModelById = (id: string): ModelInfo | null => {
	return id in modelInfo ? modelInfo[id] : null;
};

/**
 * Get a list of all model IDs
 */
export const getAllModelIds = (): string[] => {
	return Object.keys(modelInfo);
};
